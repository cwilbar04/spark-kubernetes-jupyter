{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-currency",
   "metadata": {},
   "source": [
    "# Google Cloud Platform Project Creation Workbook \n",
    " \n",
    "Use this workbook to create a google cloud project with everything needed to collect new data and host your own web app. \n",
    " \n",
    "Prerequisites:  \n",
    "+ Create Google user account  <br><br>\n",
    "+ Create your own personal Google Cloud Project and Enable Billing\n",
    "    - Enable Free Tier account by seleting \"Try it Free\" here: [Try Google Cloud Platform for free](https://cloud.google.com/cloud-console)\n",
    "    - Follow steps to activate billing found here: [Create New Billing Account](https://cloud.google.com/billing/docs/how-to/manage-billing-account#create_a_new_billing_account)\n",
    "        - Billing account is required for APIs used in this project\n",
    "        - You will not exceed the $300 free trial setting up this project but make sure to delete the project if you do not want to be charged\n",
    "        - Take note of project name created because this billing account will be used with the new project <br><br>\n",
    "+ Install and initialize Google Cloud SDK by following instructions found here: [Cloud SDK Quickstart](https://cloud.google.com/sdk/docs/quickstart) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-kansas",
   "metadata": {},
   "source": [
    "## Step 1 - Check Prequisites Successfully Completed\n",
    "Check that you have successfully installed and enabled Cloud SDK by running the config list command. If you get an error please refer to Troubleshooting steps found here [Cloud SDK Quickstart](https://cloud.google.com/sdk/docs/quickstart).  \n",
    "You should see an output that includes your account along with any other configuration setup when using gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "junior-insurance",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[accessibility]\n",
      "screen_reader = False\n",
      "[compute]\n",
      "region = us-central1\n",
      "zone = us-central1-c\n",
      "[core]\n",
      "account = cwilbar@alumni.nd.edu\n",
      "disable_usage_reporting = False\n",
      "project = spark-on-kubernetes-testing\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d5e7-4bbf-4c46-8ca7-027a1ab2008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-cooperation",
   "metadata": {},
   "source": [
    "## Step 2 - Create GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TO DO: Enter name for new project\n",
    "###### Note: Proect name must be unique across GCP. If you get error when creating project please change the project name here and try again.\n",
    "\n",
    "new_project_id = 'spark-on-kubernetes-testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "local-hollow",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Create in progress for [https://cloudresourcemanager.googleapis.com/v1/projects/spark-on-kubernetes-testing].\nWaiting for [operations/cp.6139311934009095807] to finish...\n..done.\nEnabling service [cloudapis.googleapis.com] on project [spark-on-kubernetes-testing]...\nOperation \"operations/acf.p2-291725804806-a8596c4a-3df6-43a4-aa82-cbebc437582a\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects create {new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e76a11c3-888c-4fe4-b65a-1071443c6c95",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-teacher",
   "metadata": {},
   "source": [
    "#### IMPORTANT\n",
    "*****TO DO: Navigate to [Cloud Console](https://console.cloud.google.com/), Change to new project, and enable billing following instructions found here: [Enable Billing](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-prediction",
   "metadata": {},
   "source": [
    "## Step 3 - Enable Necessary Cloud Services\n",
    "\n",
    "This project uses:\n",
    "+ Google Kubernetes Engine for a kubernetes cluster manager\n",
    "+ Google Container Registry to store spark Docker container images\n",
    "  \n",
    "List below contains all services needed at time of creation of this workbook. Please add/remove from this list if the names/necessary services have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hourly-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_services_list = [\n",
    "    'bigquery.googleapis.com',\n",
    "    'bigquerystorage.googleapis.com',\n",
    "    'cloudapis.googleapis.com',\n",
    "    'cloudbuild.googleapis.com',\n",
    "    'clouddebugger.googleapis.com',\n",
    "    'cloudtrace.googleapis.com',\n",
    "    'compute.googleapis.com',\n",
    "    'container.googleapis.com',\n",
    "    'containeranalysis.googleapis.com',\n",
    "    'containerregistry.googleapis.com',\n",
    "    'iam.googleapis.com ',\n",
    "    'iamcredentials.googleapis.com ',\n",
    "    'language.googleapis.com',\n",
    "    'oslogin.googleapis.com',\n",
    "    'servicemanagement.googleapis.com',\n",
    "    'serviceusage.googleapis.com',\n",
    "    'sql-component.googleapis.com',\n",
    "    'storage-api.googleapis.com',\n",
    "    'storage-component.googleapis.com',\n",
    "    'storage.googleapis.com'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "touched-poverty",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Operation \"operations/acf.p2-291725804806-1bda1b9f-92f0-426e-a346-bc333af49b02\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "## Services can only be enabled 20 at a time at the time of workbook creation. Use this loop to enable 20 at a time.\n",
    "for x in range(0,len(enable_services_list),20):\n",
    "    !gcloud services enable {' '.join(enable_services_list[x:(x+20)])} --project={new_project_id}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "suspended-politics",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                              TITLE\nautoml.googleapis.com             Cloud AutoML API\nbigquery.googleapis.com           BigQuery API\nbigquerystorage.googleapis.com    BigQuery Storage API\ncloudapis.googleapis.com          Google Cloud APIs\nclouddebugger.googleapis.com      Cloud Debugger API\ncloudtrace.googleapis.com         Cloud Trace API\ncontainerregistry.googleapis.com  Container Registry API\ndatastore.googleapis.com          Cloud Datastore API\nlanguage.googleapis.com           Cloud Natural Language API\nlogging.googleapis.com            Cloud Logging API\nmonitoring.googleapis.com         Cloud Monitoring API\npubsub.googleapis.com             Cloud Pub/Sub API\nrun.googleapis.com                Cloud Run Admin API\nservicemanagement.googleapis.com  Service Management API\nserviceusage.googleapis.com       Service Usage API\nsql-component.googleapis.com      Cloud SQL\nstorage-api.googleapis.com        Google Cloud Storage JSON API\nstorage-component.googleapis.com  Cloud Storage\nstorage.googleapis.com            Cloud Storage API\ntranslate.googleapis.com          Cloud Translation API\nvision.googleapis.com             Cloud Vision API\n"
     ]
    }
   ],
   "source": [
    "# Check that services were enabled\n",
    "!gcloud services list --project=simple-webapp-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-recipient",
   "metadata": {},
   "source": [
    "## Step 4 - Create Necessary Service Accounts\n",
    "\n",
    "There are two primary service accounts used in this project:  \n",
    "- **Deployment Service Account**\n",
    "    - We create this and add necessary roles below using the Cloud SDK\n",
    "    - deployer-sa@your_project_name.iam.gserviceaccount.com\n",
    "    - This account is used to deploy and test docker container and kubernetes cluster<br><br>\n",
    "- **BigQuery Service Account**\n",
    "    - We create this and add necessary roles below using the Cloud SDK\n",
    "    - bigquery-sa@your_project_name.iam.gserviceaccount.com\n",
    "    - This account is used in the container for access to big query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-climate",
   "metadata": {},
   "source": [
    "Check what service ccounts are already created (should be the two default ones described above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "joint-trinity",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DISPLAY NAME                            EMAIL                                               DISABLED\nCompute Engine default service account  291725804806-compute@developer.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "marked-costs",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created service account [deployer-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create deployer-sa \\\n",
    "    --display-name=\"Deployment Service Account\" \\\n",
    "    --description=\"Account used to deploy to Google Cloud Project\" \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "graphic-juice",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created service account [bigquery-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create bigquery-sa \\\n",
    "    --display-name=\"BigQuery Service Account\" \\\n",
    "    --description=\"Account used by Spark Containers to Connect to BigQuery\" \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-junior",
   "metadata": {},
   "source": [
    "Check service accounts were created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "attended-cattle",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DISPLAY NAME                            EMAIL                                               DISABLED\nCompute Engine default service account  291725804806-compute@developer.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-collect",
   "metadata": {},
   "source": [
    "Programatically update the roles for the new service accounts using the guide found here: [Programatic Change Access](https://cloud.google.com/iam/docs/granting-changing-revoking-access#programmatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "useful-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save policy file in directory above where the repo is saved so that it is not stored to github\n",
    "file_directory = '..\\..\\policy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "golden-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write current policy to file directory\n",
    "!gcloud projects get-iam-policy {new_project_id} --format json > {file_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-bundle",
   "metadata": {},
   "source": [
    "**If running jupyter notebook run below cell to load and modify policy file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "express-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('..\\..\\policy.json') as f:\n",
    "    policy = json.load(f)\n",
    "\n",
    "def modify_policy_add_role(policy, role, member):\n",
    "    \"\"\"Adds a new role binding to a policy.\"\"\"\n",
    "\n",
    "    binding = {\"members\": [member],\"role\": role }\n",
    "    policy[\"bindings\"].append(binding)\n",
    "    return policy\n",
    "\n",
    "members = [f'serviceAccount:deployer-sa@{new_project_id}.iam.gserviceaccount.com', \n",
    "           f'serviceAccount:bigquery-sa@{new_project_id}.iam.gserviceaccount.com']\n",
    "roles = {\n",
    "        members[0]:['roles/editor','roles/container.admin'],\n",
    "        members[1]:['roles/bigquery.dataEditor','roles/run.serviceAgent', 'roles/bigquery.user',\n",
    "                    'roles/storage.admin']}\n",
    "\n",
    "for member in members:\n",
    "    for role in roles[member]:\n",
    "        policy = modify_policy_add_role(policy, role, member)\n",
    "\n",
    "with open('..\\..\\policy.json', 'w') as json_file:\n",
    "    json.dump(policy, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "opening-command",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-testing.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.dataEditor\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-testing.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:291725804806@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-291725804806@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-291725804806@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgentUpdated IAM policy for project [spark-on-kubernetes-testing].\n",
      "\n",
      "- members:\n",
      "  - serviceAccount:service-291725804806@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-291725804806@container-analysis.iam.gserviceaccount.com\n",
      "  role: roles/containeranalysis.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-291725804806@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:291725804806-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:291725804806@cloudservices.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:deployer-sa@spark-on-kubernetes-testing.iam.gserviceaccount.com\n",
      "  - user:cwilbar@alumni.nd.edu\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:service-291725804806@gcp-sa-pubsub.iam.gserviceaccount.com\n",
      "  role: roles/pubsub.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-testing.iam.gserviceaccount.com\n",
      "  role: roles/run.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-testing.iam.gserviceaccount.com\n",
      "  role: roles/storage.admin\n",
      "etag: BwXEExnLOnE=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects set-iam-policy {new_project_id} {file_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "governmental-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove policy file \n",
    "!del {file_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-hierarchy",
   "metadata": {},
   "source": [
    "## Step 5 - Create Kubernetes Engine Cluster\n",
    "\n",
    "In order to deploy a container to kubernetes to run an application you first need to create a kubernetes engine cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "wanted-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: Change region  to your default region\n",
    "COMPUTE_REGION = 'us-central1'\n",
    "CLUSTER_NAME = 'spark-cluster'\n",
    "# COMPUTE_ZONE = 'us-central1-c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a0ea5c-e21f-44f7-8a4e-c860addfda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud compute regions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54668c69-81c1-4f6f-9f2b-32a1876f7449",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set compute/region {COMPUTE_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d28174a-6abb-4b05-94b1-2d74673d14b3",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Updated property [compute/zone].\n"
     ]
    }
   ],
   "source": [
    "# !gcloud config set compute/zone {COMPUTE_ZONE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36a14f55-1192-40f4-a028-f642d2705973",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME           LOCATION     MASTER_VERSION   MASTER_IP      MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\n",
      "WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n",
      "WARNING: The Pod address range limits the maximum size of the cluster. Please refer to https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr to learn how to optimize IP address allocation.\n",
      "WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n",
      "Creating cluster spark-cluster in us-central1...spark-cluster  us-central1  1.19.9-gke.1400  35.239.173.65  e2-medium     1.19.9-gke.1400  3          RUNNING\n",
      "\n",
      "...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Created [https://container.googleapis.com/v1/projects/spark-on-kubernetes-testing/zones/us-central1/clusters/spark-cluster].\n",
      "To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1/spark-cluster?project=spark-on-kubernetes-testing\n",
      "kubeconfig entry generated for spark-cluster.\n"
     ]
    }
   ],
   "source": [
    "# Create cluster with default settings. This may take serveral minutes\n",
    "!gcloud container clusters create-auto {CLUSTER_NAME} \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e2def4f-291f-40d1-b0f1-e3603cf5ee75",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fetching cluster endpoint and auth data.\nkubeconfig entry generated for spark-cluster.\n"
     ]
    }
   ],
   "source": [
    "# Get credentials to use when deploying to cluster\n",
    "!gcloud container clusters get-credentials {CLUSTER_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kubernetes master is running at https://35.239.173.65\nGLBCDefaultBackend is running at https://35.239.173.65/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nKubeDNS is running at https://35.239.173.65/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nKubeDNSUpstream is running at https://35.239.173.65/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy\nMetrics-server is running at https://35.239.173.65/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "!kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bbd6f08-dd35-45fa-8c90-0fb8bf2f2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "account = f'bigquery-sa@{new_project_id}.iam.gserviceaccount.com' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = f'deployer-sa@{new_project_id}.iam.gserviceaccount.com' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3cb2bb8-eea8-4795-a21a-c69bf6b320d2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "created key [9e41f235b4e9fdce3b0a37c10add384e52e86d0f] of type [json] as [sa.json] for [deployer-sa@spark-on-kubernetes-testing.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "# Download bigquery service account json file\n",
    "!gcloud iam service-accounts keys create sa.json \\\n",
    "    --iam-account={account}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dc2fc8d-2315-4326-bbb1-702db5e20fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/bigquery-credentials created\n"
     ]
    }
   ],
   "source": [
    "# Create Kubernetes Secret from file\n",
    "!kubectl create secret generic bigquery-credentials \\\n",
    "  --from-file ./sa.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9328d722-4fab-4815-badf-d8cdcda64615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove service account file from local system now that Kubernetes Secret\n",
    "!del sa.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "serviceaccount/spark created\n"
     ]
    }
   ],
   "source": [
    "# Create spark service account on Kubernetes\n",
    "!kubectl create serviceaccount spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clusterrolebinding.rbac.authorization.k8s.io/spark-role created\n"
     ]
    }
   ],
   "source": [
    "# Create role for service account to enable edit\n",
    "!kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-inspection",
   "metadata": {},
   "source": [
    "## Step 6 - Create BigQuery Dataset\n",
    "\n",
    "Your new project will need a dataset to store the data if you plan on copying/creating your own repository of data.  \n",
    "\n",
    "This has to be a unique name per project.  \n",
    "\n",
    "In my workflows I have named the dataset 'nba' but feel free to change it. Note that if you do change it, then you will also need to change the dataset name in any of the other python scripts in this project appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "empirical-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amazon_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "marked-montana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'spark-container-testing-6:amazon_reviews' successfully created.\n"
     ]
    }
   ],
   "source": [
    "#Stop and re-run if this takes more than a minute\n",
    "!bq --location=US mk --dataset \\\n",
    "--description \"Stores transformed amazon review data orginally found at https://nijianmo.github.io/amazon/index.html\" \\\n",
    "{new_project_id}:{dataset_name}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-missouri",
   "metadata": {},
   "source": [
    "## Step 7 - Build and Push Container to GCR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "congressional-sullivan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:de2dd75818e130241f519f771f57058d587994ce36d11b60328f8ee1de3fa7fc\n",
      "#1 transferring dockerfile: 32B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:3581d49c95519dc794417c23b5021871ef8eaa4ce3c3426f1dd9f86c35553cc6\n",
      "#2 transferring context: 35B done\n",
      "#2 DONE 0.1s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/ubuntu:latest\n",
      "#3 sha256:8c6bdfb121a69744f11ffa1fedfc68ec20085c2dcce567aac97a3ff72e53502d\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [ 1/16] FROM docker.io/library/ubuntu:latest\n",
      "#4 sha256:0a5f349eacf4edfd2fc1577c637ef52a2ed3280d9d5c0ab7f2e4c4052e7d6c9f\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#10 [internal] load build context\n",
      "#10 sha256:f87e9bdbcbfc66eeb7e110c17042b648c026980daed0bf0ff55a0c6a2df62bfc\n",
      "#10 transferring context: 238B done\n",
      "#10 DONE 0.0s\n",
      "\n",
      "#14 https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.0.jar\n",
      "#14 sha256:173f6ef3aef50bc006d358109ed87a9ceb38ec7b8570fecb583413853f5055c7\n",
      "#14 DONE 0.4s\n",
      "\n",
      "#17 https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.20.0.jar\n",
      "#17 sha256:89b5ffef7e782b56dfd9b6af19658592f873337a5886ddeb5a48695eb67d8fbf\n",
      "#17 DONE 0.4s\n",
      "\n",
      "#6 [ 3/16] WORKDIR /tmp\n",
      "#6 sha256:a982b8f36bb1e2aee3419bee49ad95eb41dfc77a2b98abc9aa67cbc0b1771180\n",
      "#6 CACHED\n",
      "\n",
      "#19 [13/16] RUN chmod 644 /opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.20.0.jar\n",
      "#19 sha256:cec6e26d017a2197464fe26b0baad003e5e49dc6b7fda94a212406aad7150a75\n",
      "#19 CACHED\n",
      "\n",
      "#16 [11/16] RUN chmod 644 /opt/spark/jars/gcs-connector-hadoop3-2.2.0.jar\n",
      "#16 sha256:7ec4011f781d3e01c5ab197e4102be43572fb2761cc07d2a3339d69e2cefc6bc\n",
      "#16 CACHED\n",
      "\n",
      "#8 [ 5/16] RUN echo \"alias pyspark=/opt/spark/bin/pyspark\" >> ~/.bashrc &&     echo \"alias spark-shell=/opt/spark/bin/spark-shell\" >> ~/.bashrc\n",
      "#8 sha256:96f185357a3090823b9420ce84c9b357953ea414f35e9b41f4a9c434b7e0e7bb\n",
      "#8 CACHED\n",
      "\n",
      "#18 [12/16] ADD https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.20.0.jar /opt/spark/jars\n",
      "#18 sha256:4efd7176d5a8b5f75bdfd4c3fe3ebabe6fd0fec205f29285023585431dd6ff58\n",
      "#18 CACHED\n",
      "\n",
      "#21 [15/16] WORKDIR /home/data\n",
      "#21 sha256:42a5b153479dc25899201c169852926fba2c0e7811c84d797ac2aba0b48449d6\n",
      "#21 CACHED\n",
      "\n",
      "#9 [ 6/16] RUN useradd -l -m -s /bin/bash -N -u 1000 joyvan &&     chmod g+w /etc/passwd\n",
      "#9 sha256:c32dd5d1833f25dec77190cfdecb32eaf4152f4790a2a4e61c39fdb93b75d05f\n",
      "#9 CACHED\n",
      "\n",
      "#15 [10/16] ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.0.jar /opt/spark/jars\n",
      "#15 sha256:eaca0fe0cd8369f55f864879af5d1292a71cf578d4f92b95f8957b5d4edefeef\n",
      "#15 CACHED\n",
      "\n",
      "#20 [14/16] RUN cp /opt/spark/conf/spark-defaults.conf.template /opt/spark/conf/spark-defaults.conf &&     echo spark.hadoop.google.cloud.auth.service.account.enable       true >> spark-defaults.conf &&     echo spark.hadoop.google.cloud.auth.service.account.json.keyfile        /var/secrets/google/sa.json>> spark-defaults.conf &&     echo spark.hadoop.fs.gs.AbstractFilePath.impl     com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS >> spark-defaults.conf\n",
      "#20 sha256:fd6f1bf27acec820cff7a6af650062ef518054622518848dd9132a3cf7762702\n",
      "#20 CACHED\n",
      "\n",
      "#5 [ 2/16] RUN apt-get -y update &&     apt-get install --no-install-recommends -y     curl     tini     wget     python     python3-pip     \"openjdk-14-jre-headless\"     ca-certificates-java &&     apt-get clean && rm -rf /var/lib/apt/lists/*\n",
      "#5 sha256:f1fe3dc97c3e6d269bb9adabe6b47a06f9614916cd75892be6e7b5ee3d799f40\n",
      "#5 CACHED\n",
      "\n",
      "#11 [ 7/16] COPY requirements.txt requirements.txt\n",
      "#11 sha256:167aca1508d4237a000ba8f148a66566ecd17251bd3dd587ef43a51963314482\n",
      "#11 CACHED\n",
      "\n",
      "#12 [ 8/16] RUN     pip3 install --upgrade pip &&     pip3 install -r requirements.txt &&     jupyter lab clean\n",
      "#12 sha256:9d6358eac63edefb43110f5367d3e6f22ec033237d1644d13b5c8eb211c619e9\n",
      "#12 CACHED\n",
      "\n",
      "#13 [ 9/16] WORKDIR /opt/spark/conf\n",
      "#13 sha256:5fabd45f4e8b0186233e5d66915b4c6e086b13f7d4d11fddb3341ce63cdf9a52\n",
      "#13 CACHED\n",
      "\n",
      "#7 [ 4/16] RUN wget -q $(wget -qO- https://www.apache.org/dyn/closer.lua/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz?as_json |     python3 -c \"import sys, json; content=json.load(sys.stdin); print(content['preferred']+content['path_info'])\") &&     echo \"E90B31E58F6D95A42900BA4D288261D71F6C19FA39C1CB71862B792D1B5564941A320227F6AB0E09D946F16B8C1969ED2DEA2A369EC8F9D2D7099189234DE1BE *spark-3.1.1-bin-hadoop3.2.tgz\" | sha512sum -c - &&     tar xzf \"spark-3.1.1-bin-hadoop3.2.tgz\" -C /opt --owner root --group root --no-same-owner &&     mv /opt/spark-3.1.1-bin-hadoop3.2 /opt/spark &&     rm \"spark-3.1.1-bin-hadoop3.2.tgz\"\n",
      "#7 sha256:2fcf94ef332a56b2336c8401d1677cb4c47dcb67bc1bd118fcb3d23640ec50de\n",
      "#7 CACHED\n",
      "\n",
      "#22 [16/16] COPY load_files/ load_files/\n",
      "#22 sha256:31ff6d28113225b6ab19fc076443ebeebb41ba69ab3cc8aa99d168fc74fba4c2\n",
      "#22 CACHED\n",
      "\n",
      "#23 exporting to image\n",
      "#23 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#23 exporting layers done\n",
      "#23 writing image sha256:9b00ddb7cc945ceb84efd7ab540a0af57cfb05ade93b71f841f5ca4dec5a7d88 0.0s done\n",
      "#23 naming to docker.io/library/client-mode-spark-notebook done\n",
      "#23 DONE 0.1s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t jupyterlab-pyspark cluster-mode-standalone/jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a8b079a-c3a4-48eb-bd18-7ba651f6174a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker tag jupyterlab-pyspark:latest gcr.io/{new_project_id}/jupyterlab-pyspark:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38606448-d0e9-4b90-91a1-48e190ca570f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/spark-container-testing-6/client-mode-spark-notebook]\n",
      "84657af47b81: Preparing\n",
      "5bb2d99bbe83: Preparing\n",
      "4776f7e3e7a4: Preparing\n",
      "c321536c1000: Preparing\n",
      "62b4a933f82f: Preparing\n",
      "3539f8208cfb: Preparing\n",
      "e78f6d846b48: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "73699085186c: Preparing\n",
      "234a7be13f6b: Preparing\n",
      "fd1892e6bbf9: Preparing\n",
      "b365e4125915: Preparing\n",
      "8b167cb6e70a: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "420aaa2ef3d1: Preparing\n",
      "346be19f13b0: Preparing\n",
      "935f303ebf75: Preparing\n",
      "0e64bafdc7ee: Preparing\n",
      "fd1892e6bbf9: Waiting\n",
      "b365e4125915: Waiting\n",
      "3539f8208cfb: Waiting\n",
      "e78f6d846b48: Waiting\n",
      "8b167cb6e70a: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "420aaa2ef3d1: Waiting\n",
      "73699085186c: Waiting\n",
      "234a7be13f6b: Waiting\n",
      "346be19f13b0: Waiting\n",
      "935f303ebf75: Waiting\n",
      "0e64bafdc7ee: Waiting\n",
      "84657af47b81: Pushed\n",
      "5bb2d99bbe83: Pushed\n",
      "4776f7e3e7a4: Pushed\n",
      "5f70bf18a086: Layer already exists\n",
      "e78f6d846b48: Pushed\n",
      "234a7be13f6b: Pushed\n",
      "3539f8208cfb: Pushed\n",
      "fd1892e6bbf9: Pushed\n",
      "b365e4125915: Pushed\n",
      "c321536c1000: Pushed\n",
      "346be19f13b0: Layer already exists\n",
      "935f303ebf75: Layer already exists\n",
      "0e64bafdc7ee: Layer already exists\n",
      "62b4a933f82f: Pushed\n",
      "420aaa2ef3d1: Pushed\n",
      "8b167cb6e70a: Pushed\n",
      "73699085186c: Pushed\n",
      "latest: digest: sha256:3a01b68f834c6534ccc7f2aa70ef0b55cac07f00a31d8c155f746fb3639e088b size: 4088\n"
     ]
    }
   ],
   "source": [
    "# Push to Google Container Registry - This may take a few minutes. See end of README for instuctions on how to authenticate if you get an error with the push\n",
    "!docker push gcr.io/{new_project_id}/jupyterlab-pyspark:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-sheep",
   "metadata": {},
   "source": [
    "## Step 8 - Deploy App to Cluster and Expose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39f73d4f-a3b8-4cb0-882c-8329d3b624d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = 'spark-server'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca3480d5-822b-4eab-bab4-2ac09cf75ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f'''\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: spark-server\n",
    "  labels:\n",
    "    app: spark-server\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: spark-server\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: spark-server\n",
    "    spec:\n",
    "      # The secret data is exposed to Containers in the Pod through a Volume.\n",
    "      volumes:\n",
    "      - name: secret-volume\n",
    "        secret:\n",
    "          secretName: bigquery-credentials\n",
    "      containers:\n",
    "      - image: gcr.io/{new_project_id}/client-mode-spark-notebook:latest\n",
    "        name: client-mode-spark-notebook\n",
    "        volumeMounts:\n",
    "          # name must match the volume name below\n",
    "          - name: secret-volume\n",
    "            mountPath: /var/secrets/google    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55ab4703-80e1-495b-bf90-3e5b16bc6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deployment.yaml', 'w') as file:\n",
    "    file.write(template)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e52d1c22-49fa-4e9a-9003-23186eac006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/spark-server created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06769385-3d31-42f7-a396-665de5883c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/spark-server exposed\n"
     ]
    }
   ],
   "source": [
    "!kubectl expose deployment {APP_NAME} --type LoadBalancer --port 80 --target-port 8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66474e86-f879-4b5c-8a74-7df932aea7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:              kubernetes\n",
      "Namespace:         default\n",
      "Labels:            component=apiserver\n",
      "                   provider=kubernetes\n",
      "Annotations:       <none>\n",
      "Selector:          <none>\n",
      "Type:              ClusterIP\n",
      "IP:                10.3.240.1\n",
      "Port:              https  443/TCP\n",
      "TargetPort:        443/TCP\n",
      "Endpoints:         34.136.228.198:443\n",
      "Session Affinity:  None\n",
      "Events:            <none>\n",
      "\n",
      "\n",
      "Name:                     spark-server\n",
      "Namespace:                default\n",
      "Labels:                   app=spark-server\n",
      "Annotations:              <none>\n",
      "Selector:                 app=spark-server\n",
      "Type:                     LoadBalancer\n",
      "IP:                       10.3.244.39\n",
      "LoadBalancer Ingress:     34.71.125.54\n",
      "Port:                     <unset>  80/TCP\n",
      "TargetPort:               8888/TCP\n",
      "NodePort:                 <unset>  30851/TCP\n",
      "Endpoints:                10.0.0.7:8888\n",
      "Session Affinity:         None\n",
      "External Traffic Policy:  Cluster\n",
      "Events:\n",
      "  Type    Reason                Age   From                Message\n",
      "  ----    ------                ----  ----                -------\n",
      "  Normal  EnsuringLoadBalancer  39s   service-controller  Ensuring load balancer\n",
      "  Normal  EnsuredLoadBalancer   2s    service-controller  Ensured load balancer\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-chest",
   "metadata": {},
   "source": [
    "## Step 9 - Create Cloud Storage Bucket\n",
    "\n",
    "In order to load data to BigQuery with Spark we need a temporary Cloud Storage Bucket we create below.\n",
    "\n",
    "Bucket names must be globally unique so we include the project name.\n",
    "\n",
    "When running the Jupyter Notebook to load the data you will have to set the project ID so that it uses this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3034f89-fdb9-4d99-9da7-9120169e8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb gs://amazon_reviews_bucket-{new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-aberdeen",
   "metadata": {},
   "source": [
    "## Optional - Delete Project\n",
    "\n",
    "To avoid on-going charges for everything created in this workbook run the below command to delete the project that you just created. Note it will take approximately 30 days for full completion and you will stil be charged for any charges accrued during this walkthrough. Check out [Deleting GCP Project](https://cloud.google.com/resource-manager/docs/creating-managing-projects?visit_id=637510410447506984-2569255859&rd=1#shutting_down_projects) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "surprised-tampa",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "### Uncomment code to delete project\n",
    "!gcloud projects delete {new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}