{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-currency",
   "metadata": {},
   "source": [
    "# Google Cloud Platform Project Creation Workbook \n",
    " \n",
    "Use this workbook to create a google cloud project with everything needed to create a Kubernetes Engine Cluster and deploy a spark image to be used to create a cluster mode SparkSession. \n",
    " \n",
    "Prerequisites:  \n",
    "+ Create Google user account  <br><br>\n",
    "+ Create your own personal Google Cloud Project and Enable Billing\n",
    "    - Enable Free Tier account by seleting \"Try it Free\" here: [Try Google Cloud Platform for free](https://cloud.google.com/cloud-console)\n",
    "    - Follow steps to activate billing found here: [Create New Billing Account](https://cloud.google.com/billing/docs/how-to/manage-billing-account#create_a_new_billing_account)\n",
    "        - Billing account is required for APIs used in this project\n",
    "        - You will not exceed the $300 free trial setting up this project but make sure to delete the project if you do not want to be charged\n",
    "        - Take note of project name created because this billing account will be used with the new project <br><br>\n",
    "+ Install and initialize Google Cloud SDK by following instructions found here: [Cloud SDK Quickstart](https://cloud.google.com/sdk/docs/quickstart) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-kansas",
   "metadata": {},
   "source": [
    "## Step 1 - Check Prequisites Successfully Completed\n",
    "Check that you have successfully installed and enabled Cloud SDK by running the config list command. If you get an error please refer to Troubleshooting steps found here [Cloud SDK Quickstart](https://cloud.google.com/sdk/docs/quickstart).  \n",
    "You should see an output that includes your account along with any other configuration setup when using gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "junior-insurance",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[accessibility]\n",
      "screen_reader = False\n",
      "[compute]\n",
      "region = us-central1\n",
      "[core]\n",
      "account = cwilbar@alumni.nd.edu\n",
      "disable_usage_reporting = False\n",
      "project = spark-on-kubernetes-testing\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37d5e7-4bbf-4c46-8ca7-027a1ab2008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-cooperation",
   "metadata": {},
   "source": [
    "## Step 2 - Create GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TO DO: Enter name for new project\n",
    "###### Note: Proect name must be unique across GCP. If you get error when creating project please change the project name here and try again.\n",
    "\n",
    "new_project_id = 'spark-on-kubernetes-demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud projects create {new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a11c3-888c-4fe4-b65a-1071443c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project {new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-teacher",
   "metadata": {},
   "source": [
    "#### IMPORTANT\n",
    "*****TO DO: Navigate to [Cloud Console](https://console.cloud.google.com/), Change to new project, and enable billing following instructions found here: [Enable Billing](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-prediction",
   "metadata": {},
   "source": [
    "## Step 3 - Enable Necessary Cloud Services\n",
    "\n",
    "This project uses:\n",
    "+ Google Kubernetes Engine for a kubernetes cluster manager\n",
    "+ Google Container Registry to store spark Docker container images\n",
    "  \n",
    "List below contains all services needed at time of creation of this workbook. Please add/remove from this list if the names/necessary services have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hourly-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_services_list = [\n",
    "    'bigquery.googleapis.com',\n",
    "    'bigquerystorage.googleapis.com',\n",
    "    'cloudapis.googleapis.com',\n",
    "    'cloudbuild.googleapis.com',\n",
    "    'clouddebugger.googleapis.com',\n",
    "    'cloudtrace.googleapis.com',\n",
    "    'compute.googleapis.com',\n",
    "    'container.googleapis.com',\n",
    "    'containeranalysis.googleapis.com',\n",
    "    'containerregistry.googleapis.com',\n",
    "    'iam.googleapis.com ',\n",
    "    'iamcredentials.googleapis.com ',\n",
    "    'oslogin.googleapis.com',\n",
    "    'servicemanagement.googleapis.com',\n",
    "    'serviceusage.googleapis.com',\n",
    "    'sql-component.googleapis.com',\n",
    "    'storage-api.googleapis.com',\n",
    "    'storage-component.googleapis.com',\n",
    "    'storage.googleapis.com'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-poverty",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Operation \"operations/acf.p2-601703040934-6457616c-5804-4e6e-9c54-28937d0a7e85\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "## Services can only be enabled 20 at a time at the time of workbook creation. Use this loop to enable 20 at a time.\n",
    "for x in range(0,len(enable_services_list),20):\n",
    "    !gcloud services enable {' '.join(enable_services_list[x:(x+20)])} --project={new_project_id}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suspended-politics",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                              TITLE\nautoml.googleapis.com             Cloud AutoML API\nbigquery.googleapis.com           BigQuery API\nbigquerystorage.googleapis.com    BigQuery Storage API\ncloudapis.googleapis.com          Google Cloud APIs\nclouddebugger.googleapis.com      Cloud Debugger API\ncloudtrace.googleapis.com         Cloud Trace API\ncontainerregistry.googleapis.com  Container Registry API\ndatastore.googleapis.com          Cloud Datastore API\nlanguage.googleapis.com           Cloud Natural Language API\nlogging.googleapis.com            Cloud Logging API\nmonitoring.googleapis.com         Cloud Monitoring API\npubsub.googleapis.com             Cloud Pub/Sub API\nrun.googleapis.com                Cloud Run Admin API\nservicemanagement.googleapis.com  Service Management API\nserviceusage.googleapis.com       Service Usage API\nsql-component.googleapis.com      Cloud SQL\nstorage-api.googleapis.com        Google Cloud Storage JSON API\nstorage-component.googleapis.com  Cloud Storage\nstorage.googleapis.com            Cloud Storage API\ntranslate.googleapis.com          Cloud Translation API\nvision.googleapis.com             Cloud Vision API\n"
     ]
    }
   ],
   "source": [
    "# Check that services were enabled\n",
    "!gcloud services list --project=simple-webapp-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-recipient",
   "metadata": {},
   "source": [
    "## Step 4 - Create Necessary Service Accounts\n",
    "\n",
    "There are two primary service accounts used in this project:  \n",
    "- **Deployment Service Account**\n",
    "    - We create this and add necessary roles below using the Cloud SDK\n",
    "    - deployer-sa@your_project_name.iam.gserviceaccount.com\n",
    "    - This account is used to deploy and test docker container and kubernetes cluster<br><br>\n",
    "- **BigQuery Service Account**\n",
    "    - We create this and add necessary roles below using the Cloud SDK\n",
    "    - bigquery-sa@your_project_name.iam.gserviceaccount.com\n",
    "    - This account is used in the container for access to big query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-climate",
   "metadata": {},
   "source": [
    "Check what service ccounts are already created (should be the two default ones described above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "joint-trinity",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DISPLAY NAME                            EMAIL                                               DISABLED\nCompute Engine default service account  601703040934-compute@developer.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "marked-costs",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created service account [deployer-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create deployer-sa \\\n",
    "    --display-name=\"Deployment Service Account\" \\\n",
    "    --description=\"Account used to deploy to Google Cloud Project\" \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "graphic-juice",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Created service account [bigquery-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create bigquery-sa \\\n",
    "    --display-name=\"BigQuery Service Account\" \\\n",
    "    --description=\"Account used by Spark Containers to Connect to BigQuery\" \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-junior",
   "metadata": {},
   "source": [
    "Check service accounts were created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attended-cattle",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DISPLAY NAME                            EMAIL                                                         DISABLED\nCompute Engine default service account  601703040934-compute@developer.gserviceaccount.com            False\nBigQuery Service Account                bigquery-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com  False\nDeployment Service Account              deployer-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-collect",
   "metadata": {},
   "source": [
    "Programatically update the roles for the new service accounts using the guide found here: [Programatic Change Access](https://cloud.google.com/iam/docs/granting-changing-revoking-access#programmatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "useful-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save policy file in directory above where the repo is saved so that it is not stored to github\n",
    "file_directory = '..\\..\\policy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "golden-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write current policy to file directory\n",
    "!gcloud projects get-iam-policy {new_project_id} --format json > {file_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-bundle",
   "metadata": {},
   "source": [
    "**If running jupyter notebook run below cell to load and modify policy file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "express-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('..\\..\\policy.json') as f:\n",
    "    policy = json.load(f)\n",
    "\n",
    "def modify_policy_add_role(policy, role, member):\n",
    "    \"\"\"Adds a new role binding to a policy.\"\"\"\n",
    "\n",
    "    binding = {\"members\": [member],\"role\": role }\n",
    "    policy[\"bindings\"].append(binding)\n",
    "    return policy\n",
    "\n",
    "members = [f'serviceAccount:deployer-sa@{new_project_id}.iam.gserviceaccount.com', \n",
    "           f'serviceAccount:bigquery-sa@{new_project_id}.iam.gserviceaccount.com']\n",
    "roles = {\n",
    "        members[0]:['roles/editor','roles/container.admin'],\n",
    "        members[1]:['roles/bigquery.dataEditor','roles/run.serviceAgent', 'roles/bigquery.user',\n",
    "                    'roles/storage.admin']}\n",
    "\n",
    "for member in members:\n",
    "    for role in roles[member]:\n",
    "        policy = modify_policy_add_role(policy, role, member)\n",
    "\n",
    "with open('..\\..\\policy.json', 'w') as json_file:\n",
    "    json.dump(policy, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "opening-command",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.dataEditorUpdated IAM policy for project [spark-on-kubernetes-demo].\n",
      "\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:601703040934@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-601703040934@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-601703040934@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:deployer-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com\n",
      "  role: roles/container.admin\n",
      "- members:\n",
      "  - serviceAccount:service-601703040934@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-601703040934@container-analysis.iam.gserviceaccount.com\n",
      "  role: roles/containeranalysis.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-601703040934@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:601703040934-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:601703040934@cloudservices.gserviceaccount.com\n",
      "  - serviceAccount:deployer-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - user:cwilbar@alumni.nd.edu\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:service-601703040934@gcp-sa-pubsub.iam.gserviceaccount.com\n",
      "  role: roles/pubsub.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com\n",
      "  role: roles/run.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com\n",
      "  role: roles/storage.admin\n",
      "etag: BwXEJR-s4Ck=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects set-iam-policy {new_project_id} {file_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governmental-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove policy file \n",
    "!del {file_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-hierarchy",
   "metadata": {},
   "source": [
    "## Step 5 - Create Kubernetes Engine Cluster\n",
    "\n",
    "In order to deploy a container to kubernetes to run an application you first need to create a kubernetes engine cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "wanted-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: Change region  to your default region\n",
    "COMPUTE_REGION = 'us-central1'\n",
    "CLUSTER_NAME = 'spark-cluster'\n",
    "# COMPUTE_ZONE = 'us-central1-c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0ea5c-e21f-44f7-8a4e-c860addfda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud compute regions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54668c69-81c1-4f6f-9f2b-32a1876f7449",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set compute/region {COMPUTE_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28174a-6abb-4b05-94b1-2d74673d14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud config set compute/zone {COMPUTE_ZONE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a14f55-1192-40f4-a028-f642d2705973",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME           LOCATION     MASTER_VERSION   MASTER_IP       MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\n",
      "spark-cluster  us-central1  1.19.9-gke.1400  35.224.220.234  e2-medium     1.19.9-gke.1400  3          RUNNING\n",
      "WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n",
      "WARNING: The Pod address range limits the maximum size of the cluster. Please refer to https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr to learn how to optimize IP address allocation.\n",
      "WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n",
      "Creating cluster spark-cluster in us-central1...\n",
      "..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Created [https://container.googleapis.com/v1/projects/spark-on-kubernetes-demo/zones/us-central1/clusters/spark-cluster].\n",
      "To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1/spark-cluster?project=spark-on-kubernetes-demo\n",
      "kubeconfig entry generated for spark-cluster.\n"
     ]
    }
   ],
   "source": [
    "# Create cluster with default settings. This may take serveral minutes\n",
    "!gcloud container clusters create-auto {CLUSTER_NAME} \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2def4f-291f-40d1-b0f1-e3603cf5ee75",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fetching cluster endpoint and auth data.\nkubeconfig entry generated for spark-cluster.\n"
     ]
    }
   ],
   "source": [
    "# Get credentials to use when deploying to cluster\n",
    "!gcloud container clusters get-credentials {CLUSTER_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kubernetes master is running at https://35.224.220.234\nGLBCDefaultBackend is running at https://35.224.220.234/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nKubeDNS is running at https://35.224.220.234/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nKubeDNSUpstream is running at https://35.224.220.234/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy\nMetrics-server is running at https://35.224.220.234/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
     ]
    }
   ],
   "source": [
    "!kubectl cluster-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bbd6f08-dd35-45fa-8c90-0fb8bf2f2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "account = f'bigquery-sa@{new_project_id}.iam.gserviceaccount.com' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3cb2bb8-eea8-4795-a21a-c69bf6b320d2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "created key [80b06e0d229e20b45f669f832ff5dd491abdf7dd] of type [json] as [sa.json] for [bigquery-sa@spark-on-kubernetes-demo.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "# Download bigquery service account json file\n",
    "!gcloud iam service-accounts keys create sa.json \\\n",
    "    --iam-account={account}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dc2fc8d-2315-4326-bbb1-702db5e20fe5",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "secret/bigquery-credentials created\n"
     ]
    }
   ],
   "source": [
    "# Create Kubernetes Secret from file\n",
    "!kubectl create secret generic bigquery-credentials \\\n",
    "  --from-file ./sa.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9328d722-4fab-4815-badf-d8cdcda64615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove service account file from local system now that Kubernetes Secret\n",
    "!del sa.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "serviceaccount/spark created\n"
     ]
    }
   ],
   "source": [
    "# Create spark service account on Kubernetes\n",
    "!kubectl create serviceaccount spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clusterrolebinding.rbac.authorization.k8s.io/spark-role created\n"
     ]
    }
   ],
   "source": [
    "# Create role for service account to enable edit\n",
    "!kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-missouri",
   "metadata": {},
   "source": [
    "## Step 7 - Build and Push Container to GCR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "congressional-sullivan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:f9f399f3d4f6dd7181072e865f382bad519a64a3cb52e8fe789e22c642848889\n",
      "#1 transferring dockerfile: 32B 0.0s done\n",
      "#1 DONE 0.3s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:72472a21fa9b0f37e1b807e671642cf3d43abfea18f554e02289d987472c31e2\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.3s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/base:latest\n",
      "#3 sha256:f9acfca7c619f83f3a6772cbdc63caa09becc43fe1028edcee2a2990821935c7\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [1/9] FROM docker.io/library/base\n",
      "#4 sha256:46b25337b6497e2a04bb43db7e2ddfd626590fa67e24801204556a1c358dfb18\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#9 [internal] load build context\n",
      "#9 sha256:a041496ec171bb8076518906f4aa587bd72e2c26fec588600960d56095e00894\n",
      "#9 transferring context: 79B done\n",
      "#9 DONE 0.1s\n",
      "\n",
      "#11 [7/9] RUN pip3 install --upgrade pip &&     pip3 install -r requirements.txt &&     jupyter lab clean\n",
      "#11 sha256:32d4e225f61b4e1291bd8a6970ee752b756d610a49e3e26a967cf9c9a9fab2d9\n",
      "#11 CACHED\n",
      "\n",
      "#10 [6/9] COPY requirements.txt requirements.txt\n",
      "#10 sha256:95453ce06362494e3b6d094680d3a3a3c1f5627283f031cd0eb96cf1879d1f56\n",
      "#10 CACHED\n",
      "\n",
      "#8 [5/9] WORKDIR /tmp\n",
      "#8 sha256:aeefd4461265d00ae2c6a313dd844a1a4aced17a81a84ec82bde6dc820c2aabc\n",
      "#8 CACHED\n",
      "\n",
      "#12 [8/9] COPY starter-notebook.ipynb /opt/workspace/starter-notebook.ipynb\n",
      "#12 sha256:aa48e6e49096091ee3f92facb430efaab9df11328009ddab92a802b9f4fc23a1\n",
      "#12 CACHED\n",
      "\n",
      "#5 [2/9] RUN apt-get update -y &&     apt-get install -y python3-pip     apt-transport-https     ca-certificates gnupg &&    apt-get clean\n",
      "#5 sha256:ba91bd28112ff1764aecb4ee22d173e90f3df7139fdf5dc2410799ed93d60645\n",
      "#5 CACHED\n",
      "\n",
      "#6 [3/9] RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add -\n",
      "#6 sha256:16241ba4e9eb6bbda7df3429dd892428f88e824223be9f303bee2a924fdcb661\n",
      "#6 CACHED\n",
      "\n",
      "#7 [4/9] RUN apt-get update -y &&     apt-get install google-cloud-sdk kubectl -y &&     apt-get clean\n",
      "#7 sha256:e6e6040976e85c4a7e7519a595cb8796e854231faaabba7ad0dd3df8fc436f76\n",
      "#7 CACHED\n",
      "\n",
      "#13 [9/9] WORKDIR /opt/workspace\n",
      "#13 sha256:58441d4ab7c24056fba5d0e4d4ead8c10126531be027b721915094bde391e53c\n",
      "#13 CACHED\n",
      "\n",
      "#14 exporting to image\n",
      "#14 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#14 exporting layers done\n",
      "#14 writing image sha256:fae50a936214d78a75887ef7387744202c71398e41a7c617d2304b85dba58e6b\n",
      "#14 writing image sha256:fae50a936214d78a75887ef7387744202c71398e41a7c617d2304b85dba58e6b done\n",
      "#14 naming to docker.io/library/jupyterlab-pyspark done\n",
      "#14 DONE 0.1s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t jupyterlab-pyspark cluster-mode-standalone/jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n#1 sha256:61651b4a19e1fc013b56a6706fce259d726c27281e6fdc914f8d94798e4910fe\n#1 transferring dockerfile: 32B done\n#1 DONE 0.1s\n\n#2 [internal] load .dockerignore\n#2 sha256:c19ceea3664cea9970b438ae3c6e9b6f3b89c5b3928e14b0746c32a5277e93e0\n#2 transferring context: 2B done\n#2 DONE 0.1s\n\n#3 [internal] load metadata for docker.io/library/base:latest\n#3 sha256:f9acfca7c619f83f3a6772cbdc63caa09becc43fe1028edcee2a2990821935c7\n#3 DONE 0.0s\n\n#4 [ 1/10] FROM docker.io/library/base\n#4 sha256:46b25337b6497e2a04bb43db7e2ddfd626590fa67e24801204556a1c358dfb18\n#4 DONE 0.0s\n\n#8 https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.0.jar\n#8 sha256:173f6ef3aef50bc006d358109ed87a9ceb38ec7b8570fecb583413853f5055c7\n#8 DONE 0.5s\n\n#11 https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.20.0.jar\n#11 sha256:89b5ffef7e782b56dfd9b6af19658592f873337a5886ddeb5a48695eb67d8fbf\n#11 DONE 0.6s\n\n#12 [ 7/10] ADD https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.20.0.jar /opt/spark/jars\n#12 sha256:0d8108a9dca71531dd9c431a69b539139dc017c082e09fe79742b2992df437ae\n#12 CACHED\n\n#6 [ 3/10] RUN curl https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz -o spark.tgz &&     tar -xf spark.tgz -C /opt --owner root --group root --no-same-owner &&     mv /opt/spark-3.1.1-bin-hadoop3.2 /opt/spark &&     rm \"spark.tgz\" &&     echo \"alias pyspark=/opt/spark/bin/pyspark\" >> ~/.bashrc &&     echo \"alias spark-shell=/opt/spark/bin/spark-shell\" >> ~/.bashrc &&     mkdir /opt/spark/logs\n#6 sha256:35bf790577f94e4cedce2bb976518d3834ea1cb5ef814a691f8aaacee8f5bd7f\n#6 CACHED\n\n#5 [ 2/10] WORKDIR /tmp\n#5 sha256:c9fb391b0df1a0180f44436e9a87c0f69d80a4189052adb8681821f6ed9e886c\n#5 CACHED\n\n#7 [ 4/10] WORKDIR /opt/spark/conf\n#7 sha256:6fcc41b5727a88d8caab5e99d7c6bbf4c07b959eb37d546202a75934b24a7412\n#7 CACHED\n\n#10 [ 6/10] RUN chmod 644 /opt/spark/jars/gcs-connector-hadoop3-2.2.0.jar\n#10 sha256:d6007463b7d62fee7e0670315039e65a6512a132977013674406dfabeb8207c6\n#10 CACHED\n\n#9 [ 5/10] ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.0.jar /opt/spark/jars\n#9 sha256:1d2bf2ecc3ca384a08ac1c1dc9eb8ed158e632cee17739ec3c4ccc93dfd14fe3\n#9 CACHED\n\n#13 [ 8/10] RUN chmod 644 /opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.20.0.jar\n#13 sha256:5a89b218360c6ab149e1256328a73815dec3cadb9a1682fc61141a4e2861c473\n#13 CACHED\n\n#14 [ 9/10] RUN cp /opt/spark/conf/spark-defaults.conf.template /opt/spark/conf/spark-defaults.conf &&     echo spark.hadoop.google.cloud.auth.service.account.enable       true >> spark-defaults.conf &&     echo spark.hadoop.google.cloud.auth.service.account.json.keyfile        /opt/workspace/sa.json>> spark-defaults.conf &&     echo spark.hadoop.fs.gs.AbstractFilePath.impl     com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS >> spark-defaults.conf\n#14 sha256:caec5df6ddca5e5f9d9cea0ac5f8e4ec246ce16459aaad7d34424b7404039f33\n#14 CACHED\n\n#15 [10/10] WORKDIR /opt/spark\n#15 sha256:1826438db56e4baa596bf80a46c18840a137154468c11e719c3475fef7cae285\n#15 CACHED\n\n#16 exporting to image\n#16 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n#16 exporting layers done\n#16 writing image sha256:d7d2969ca6d51202a27e10f5c9ae979acf0ccf174bd138d88ed1312715590734 0.0s done\n#16 naming to docker.io/library/spark-base done\n#16 DONE 0.1s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t spark-base cluster-mode-standalone\\spark-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag spark-base:latest gcr.io/{new_project_id}/spark-cluster-spark-base:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38606448-d0e9-4b90-91a1-48e190ca570f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The push refers to repository [gcr.io/spark-on-kubernetes-demo/spark-cluster-spark-base]\n5f70bf18a086: Preparing\nde8fab2b7fdf: Preparing\nfde9e65a648f: Preparing\n838aabcb60dd: Preparing\neb650d0160b4: Preparing\n564be568a877: Preparing\n5f70bf18a086: Preparing\n1427caa01bbe: Preparing\n5f70bf18a086: Preparing\nc0843df22a5a: Preparing\n346be19f13b0: Preparing\n935f303ebf75: Preparing\n0e64bafdc7ee: Preparing\n564be568a877: Waiting\n935f303ebf75: Waiting\n0e64bafdc7ee: Waiting\n1427caa01bbe: Waiting\nc0843df22a5a: Waiting\n346be19f13b0: Waiting\n5f70bf18a086: Layer already exists\neb650d0160b4: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\n838aabcb60dd: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\nde8fab2b7fdf: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\nfde9e65a648f: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\n346be19f13b0: Layer already exists\n564be568a877: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\n1427caa01bbe: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\n0e64bafdc7ee: Layer already exists\n935f303ebf75: Layer already exists\nc0843df22a5a: Mounted from spark-on-kubernetes-testing/spark-cluster-spark-base\nlatest: digest: sha256:364c7487b70d61552317150e8caf81e9d63525ae75a4776744719eb172dae784 size: 3041\n"
     ]
    }
   ],
   "source": [
    "# Push to Google Container Registry - This may take a few minutes.\n",
    "!docker push gcr.io/{new_project_id}/spark-cluster-spark-base:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-sheep",
   "metadata": {},
   "source": [
    "## Step 8 - Run Jupyterlab Image and Test Code\n",
    "\n",
    "You now how a local Jupyterlab Docker image. Run the code below in the command line outside of this interactive session to launch Jupyer Lab with a starter notebook with the code to create a SparkSession to your new server. You will first need to create a new file named untitled.txt and copy the output of sa.json. Make sure you don't upload the sa.json file to github and you delete it once you are done with it.\n",
    "\n",
    "Once you have the file, run the first three cells and find the Kubernetes IP in the \"Kubernetes control plane is running at:\" line and copy this in to the SparkSession command as the spark master IP. \n",
    "  \n",
    "One weird bug I have found is that you first need to create a SparkSession that is not a cluster mode kubernetes session and stop it. Then you can create a Kubernetes mode spark session.\n",
    "\n",
    "At this point navigate to the Cloud Console and check out Kubernetes Engine Workloads. You will see pods attempt to be created but continuously failing. This is as far as I was able to get to get this working. Make sure you stop the session and then delete the project."
   ]
  },
  {
   "source": [
    "## COPY AND RUN IN CMD SESSION NOT IN JUPYTER NOTEBOOK ##\n",
    "docker run -it --name jupyterlab-pyspark --rm -p 8888:8888 jupyterlab-pyspark\n",
    "  \n",
    "Click on 127.0.0.0 link to launch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to download the service account json for the deployer account\n",
    "account = f'deployer-sa@{new_project_id}.iam.gserviceaccount.com' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download bigquery service account json file\n",
    "!gcloud iam service-accounts keys create sa.json \\\n",
    "    --iam-account={account}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-aberdeen",
   "metadata": {},
   "source": [
    "## Optional - Delete Project\n",
    "\n",
    "To avoid on-going charges for everything created in this workbook run the below command to delete the project that you just created. Note it will take approximately 30 days for full completion and you will stil be charged for any charges accrued during this walkthrough. Check out [Deleting GCP Project](https://cloud.google.com/resource-manager/docs/creating-managing-projects?visit_id=637510410447506984-2569255859&rd=1#shutting_down_projects) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment code to delete project\n",
    "!gcloud projects delete {new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}